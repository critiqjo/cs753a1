%%%%%%%%%%%%%%%%%%%%%%
%% Template adapted from: 
%% https://www.overleaf.com/latex/examples/introduction-to-electrical-engineering-example-assignment-template/pqvbrbjtjcqq#.WH72rTXWBC0
%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper,titlepage]{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage{enumerate}

\usepackage{amsmath, amssymb, amsfonts, amsthm, mathtools}
\usepackage{microtype} %improves the spacing between words and letters
\usepackage{graphicx}

% -----------------------------------------------
\DeclareMathOperator*{\argmax}{arg\,max} % Jan Hlavacek
% -----------------------------------------------


% ------------------------------------------------
%% COLOR DEFINITIONS
% ------------------------------------------------
\usepackage[svgnames]{xcolor} % Enabling mixing colors and color's call by 'svgnames'
% ------------------------------------------------
\definecolor{MyBlue}{rgb}{0.2,0.4,0.6} %mix personal color
\newcommand{\blue}{\color{MyBlue} \usefont{OT1}{lmss}{m}{n}}
\newcommand{\blueb}{\color{MyBlue} \usefont{OT1}{lmss}{b}{n}}
% ------------------------------------------------


% ------------------------------------------------
%% FONTS AND COLORS
% ------------------------------------------------
\usepackage{sectsty}
%set section/subsections HEADINGS font and color
\sectionfont{\color{MyBlue}}  % sets colour of sections
\subsectionfont{\color{MyBlue}}  % sets colour of sections

\usepackage{hyperref}
\hypersetup{colorlinks=true}

% ------------------------------------------------
%% TITLE
% ------------------------------------------------
\title{\blue CS 753: Automatic Speech Recognition \\ \blueb Assignment 1}
\author{John C F \\ \small 14305R006}
\date{\today}
% ------------------------------------------------

\begin{document}
\maketitle
%\tableofcontents{}
\newpage

% ------------------------------------------------

\section*{Problem 1 Solution}
\subsection*{Part (a)}

\addtolength{\jot}{0.25em}
\begin{enumerate}
    \item (i) To prove $\Pr(A | B) > \Pr(A) \iff \Pr(B | A) > \Pr(B)$.
        \begin{align*}
            \Pr(A | B) &> \Pr(A) \\
            \frac{\Pr(A \cap B)}{\Pr(B)} &> \Pr(A) \\
            \frac{\Pr(A \cap B)}{\Pr(A)} &> \Pr(B) \\
            \therefore~\Aboxed{\Pr(B | A) &> \Pr(A)}
        \end{align*}

        (ii) To prove $\Pr(A | B) > \Pr(A) \iff \Pr(A^c | B^c) > \Pr(A^c)$.
        \begin{align*}
            \Pr(A | B) &> \Pr(A) \\
            \frac{\Pr(A \cap B)}{\Pr(B)} &> \Pr(A) \\
            \Pr(A \cap B) &> \Pr(A) \Pr(B) \\
            1 - \Pr(A \cap B) &< 1 - \Pr(A) \Pr(B) \\
            \Pr((A \cap B)^c) &< 1 - (1 - \Pr(A^c)) (1 - \Pr(B^c)) \\
            \Pr(A^c \cup B^c) &< 1 - (1 - \Pr(A^c) - \Pr(B^c) + \Pr(A^c) \Pr(B^c)) \\
            \Pr(A^c) + \Pr(B^c) - \Pr(A^c \cap B^c) &< \Pr(A^c) + \Pr(B^c) - \Pr(A^c) \Pr(B^c) \\
            \Pr(A^c \cap B^c) &> \Pr(A^c) \Pr(B^c) \\
            \frac{\Pr(A^c \cap B^c)}{\Pr(B^c)} &> \Pr(A^c) \\
            \therefore~\Aboxed{\Pr(A^c | B^c) &> \Pr(A^c)}
        \end{align*}

    \item Let $S_b$ denote sending bit $b$, and $R_b$ denote receiving bit $b$.
        The problem is to prove that $\Pr(S_0 | R_0) > \Pr(S_0)$ given that the
        probability of the bit being flipped by the channel is $\epsilon < 1/2$.
        Given:
        \begin{align*}
            \Pr(S_0) &= p & \Pr(R_0 | S_0) &= 1 - \epsilon \\
            \Pr(S_1) &= 1 - p & \Pr(R_0 | S_1) &= \epsilon \\
        \end{align*}
        Now,
        \begin{align}
            \Pr(S_0 | R_0) &= \frac{\Pr(R_0 \cap S_0)}{\Pr(R_0)} \nonumber \\
                           &= \frac{\Pr(R_0 | S_0) \Pr(S_0)}
                                   {\Pr(R_0 | S_0)\Pr(S_0) + \Pr(R_0 | S_1)\Pr(S_1)} \nonumber \\
                           &= \frac{(1 - \epsilon) p}
                                   {(1 - \epsilon) p + \epsilon (1 - p)} \nonumber \\
                           &= p \cdot \frac{1}{p + \frac{\epsilon}{1 - \epsilon} \cdot (1 - p)} \label{eq:s0r0} \\
                           &< p \nonumber
        \end{align}
        Because
        \begin{align*}
                        &\epsilon < 1/2 \\
            \Rightarrow~&\frac{\epsilon}{1 - \epsilon} < 1 \\
            \Rightarrow~&p + \frac{\epsilon}{1 - \epsilon} \cdot (1 - p) < 1 \\
            \Rightarrow~& \Pr(S_0 | R_0) > p \tag{from \ref{eq:s0r0}} \\
            \therefore~\Aboxed{&\Pr(S_0 | R_0) > \Pr(S_0)}
        \end{align*}
\end{enumerate}


\subsection*{Part (b)}

If $p_k$ is the probability that the ant gets the sugar if it starts $k$
centimeters to the right of the sugar, then our aim is to find $p_1$.
Now, for $0 < k \le n$,
\begin{align}
    p_0 &= 1 \nonumber \\
    p_{n+1} &= 0 \label{eq:p_n1} \\
    p_k &= 0.5 p_{k-1} + 0.5 p_{k+1} \label{eq:p_k}
\end{align}

For $k = n$ in equation \ref{eq:p_k},
\begin{align}
    p_n &= \frac{1}{2} p_{n-1} \label{eq:p_n}
\end{align}

For $k = n - 1$ in equation \ref{eq:p_k} and $n > 1$,
\begin{align*}
    p_{n-1} &= \frac{1}{2} p_{n-2} + \frac{1}{2} p_n \\
            &= \frac{1}{2} p_{n-2} + \frac{1}{4} p_{n-1} \tag{from \ref{eq:p_n}} \\
    \Rightarrow \frac{3}{4} p_{n-1} &= \frac{1}{2} p_{n-2} \\
    \Rightarrow p_{n-1} &= \frac{2}{3} p_{n-2}
\end{align*}

Similarly, for $k = n - 2$ and $n > 2$,
\begin{align*}
    p_{n-2} &= \frac{1}{2} p_{n-3} + \frac{1}{2} p_{n-1} \\
    \Rightarrow p_{n-2} &= \frac{3}{4} p_{n-3}
\end{align*}

And for $k = n - 3$ and $n > 3$, we get
\begin{align*}
    p_{n-3} &= \frac{4}{5} p_{n-4}
\end{align*}

This pattern can be generalized as, for $n > c \ge 0$,
\begin{equation}\label{eq:pnk_rec}
    p_{n-c} = \frac{c + 1}{c + 2} \cdot p_{n-c-1}
\end{equation}

For $c = n - 2$ in equation \ref{eq:pnk_rec} and $n \ge 2$,
\begin{align}
    p_2 &= \frac{n - 1}{n} p_1 \label{eq:p_2}
\end{align}

For $k = 1$ in equation \ref{eq:p_k},
\begin{align}
    p_1 &= \frac{1}{2} p_0 + \frac{1}{2} p_2 \nonumber \\
    \Rightarrow 2 p_1 &= 1 + p_2 \label{eq:p_1}
\end{align}

Equation \ref{eq:p_2} in equation \ref{eq:p_1}, for $n \ge 2$,
\begin{align*}
    2p_1 &= 1 + \frac{n - 1}{n} p_1 \\
    \Rightarrow \frac{n + 1}{n} p_1 &= 1 \\
    \Rightarrow~\Aboxed{p_1 &= \frac{n}{n + 1}}
\end{align*}

For $n = 1$, $p_1 = 1/2$, since the ant can only take a single random move.
Thus, the above equation holds for $n \ge 1$.

% ------------------------------------------------

\section*{Problem 2 Solution}

\subsection*{Part (c)}

Normalize the weights given due to unigram frequencies to be between 0 and 1.
This way, edit distance will always take precedence.

\subsection*{Part (d)}

\begin{itemize}
    \item Lower the cost of changing letters that sounds similar (e.g. c to k,
        o to au etc.).
    \item Lower the cost of deletion of one of the consonants that are next to
        each other in QWERTY layout, when appearing together (a mistype).
    \item Lower the cost of repeating a consonant when in the middle of a word
        (esp. t to tt).
\end{itemize}

% ------------------------------------------------

\section*{Problem 3 Solution}
\subsection*{Part (a)}
(i) To calculate: $\Pr(q_{t+1} = k | q_t = j, o_1, \cdots o_T)$
\begin{align*}
    ~~ &= \Pr(q_{t+1} = k | q_t = j, o_{t+1}, \cdots o_T) \tag{Markov property} \\
       &= \frac{\Pr(q_{t+1} = k, o_1, \cdots o_T | q_t = j)}
               {\Pr(o_{t+1}, \cdots o_T | q_t = j)} \\
       &= \frac{\Pr(o_{t+1}, \cdots o_T | q_{t+1} = k, q_t = j) \cdot \Pr(q_{t+1} = k | q_t = j)}
               {\beta_t(j)} \\
       &= \frac{\Pr(o_{t+1}, \cdots o_T | q_{t+1} = k) \cdot a_{jk}}
               {\beta_t(j)} \tag{Markov prop.} \\
       &= \frac{\Pr(o_{t+2}, \cdots o_T | o_{t+1}, q_{t+1} = k) \cdot \Pr(o_{t+1} | q_{t+1} = k) \cdot a_{jk}}
               {\beta_t(j)} \\
       &= \frac{\Pr(o_{t+2}, \cdots o_T | q_{t+1} = k) \cdot b_k(o_{t+1}) \cdot a_{jk}}
               {\beta_t(j)} \tag{Markov prop.} \\
       &= \frac{\beta_{t+1}(k) \cdot b_k(o_{t+1}) \cdot a_{jk}}
               {\beta_t(j)}
\end{align*}
For the sake of next part, let's define a function $p_t(i, j)$ as follows:
\begin{align*}
    p_t(i, j) &= \Pr(q_{t+1} = j | q_t = i, o_1, \cdots o_T) \\
              &= a_{ij} \cdot b_j(o_{t+1}) \cdot \frac{\beta_{t+1}(j)}{\beta_t(i)}
\end{align*}
(ii) To calculate: $\Pr(q_{t-1} = i, q_t = j, q_{t+1} = k | o_1, \cdots o_T)$
\begin{align*}
    ~~ &= \Pr(q_{t+1} = k | q_{t-1} = i, q_t = j, o_1, \cdots o_T) \cdot \Pr(q_{t-1} = i, q_t = j | o_1, \cdots o_T) \\
       &= \Pr(q_{t+1} = k | q_t = j, o_1, \cdots o_T) \cdot \Pr(q_{t-1} = i, q_t = j | o_1, \cdots o_T) \tag{Markov prop.} \\
       &= p_t(j, k) \cdot \Pr(q_t = j | q_{t-1} = i, o_1, \cdots o_T) \cdot \Pr(q_{t-1} = i | o_1, \cdots o_T) \\
       &= p_t(j, k) \cdot p_{t-1}(i, j) \cdot \frac{\Pr(q_{t-1} = i, o_1, \cdots o_T)}{\Pr(o_1, \cdots o_T)} \\
       &= p_t(j, k) \cdot p_{t-1}(i, j) \cdot
          \frac{\Pr(o_t, \cdots o_T | o_1, \cdots o_{t-1}, q_{t-1} = i) \cdot
                \Pr(o_1, \cdots o_{t-1}, q_{t-1} = i)}
               {\sum_{h \in Q}\Pr(o_1, \cdots o_T | q_T = h) \cdot \Pr(q_T = h)} \\
       &= p_t(j, k) \cdot p_{t-1}(i, j) \cdot
          \frac{\Pr(o_t, \cdots o_T | q_{t-1} = i) \cdot \alpha_{t-1}(i)}
               {\sum_{h \in Q}\Pr(o_1, \cdots o_T, q_T = h)} \\
       &= p_t(j, k) \cdot p_{t-1}(i, j) \cdot
          \frac{\beta_{t-1}(i) \cdot \alpha_{t-1}(i)}{\sum_{h \in Q}\alpha_T(h)}
\end{align*}
Now, expanding $p_t(j, k)$ and $p_{t-1}(i, j)$,
\begin{align*}
    ~~ &= a_{jk} \cdot b_k(o_{t+1}) \cdot \frac{\beta_{t+1}(k)}{\beta_t(j)} \cdot
          a_{ij} \cdot b_j(o_t) \cdot \frac{\beta_t(j)}{\beta_{t-1}(i)} \cdot
          \frac{\beta_{t-1}(i) \cdot \alpha_{t-1}(i)}{\sum_{h \in Q}\alpha_T(h)} \\
       &= a_{ij} \cdot a_{jk} \cdot b_k(o_{t+1}) \cdot b_j(o_t) \cdot
          \frac{\beta_{t+1}(k) \cdot \alpha_{t-1}(i)}{\sum_{h \in Q}\alpha_T(h)} \\
\end{align*}

\subsection*{Part (b)}

\textbf{Recursion:} For all $1 < t \le T$,
\begin{align*}
    v_{t-1, \max} &= \max_{j=1}^N v_{t-1}(j) \\
    bt_{t-1, \max} &= \argmax_{j=1}^N v_{t-1}(j) % FIXME
\end{align*}
And for all $1 \le j \le N$,
\begin{align*}
    v_t(j) &=
    \begin{dcases}
        v_{t-1, \max} \; q \; b_j(o_t)
            & {\rm if~} bt_{t-1, \max} \ne j {\rm~and~} v_{t-1, \max} \; q > v_{t-1}(j) \; p \\
        v_{t-1}(j) \; p \; b_j(o_t)
            & \textrm{otherwise}
    \end{dcases} \\
    bt_t(j) &=
    \begin{dcases}
        bt_{t-1,\max}
            & {\rm if~} bt_{t-1, \max} \ne j {\rm~and~} v_{t-1, \max} \; q > v_{t-1}(j) \; p \\
        j
            & \textrm{otherwise}
    \end{dcases}
\end{align*}

\noindent \textbf{Initialization} and \textbf{Termination} steps remain unchanged.

\subsection*{Part (c)}

\begin{enumerate}
    \item \textbf{Initialization:} For all $1 \le j \le N$,
        \begin{align*}
            v_1(j) &= a_{0j} \; b_j(o1) \\
            bt_1(j) &= 0 \\
            c_1(j) &= 0
        \end{align*}
    \item \textbf{Recursion:} For all $1 < t \le T$ and $1 \le j \le N$,
        If $c_{t-1}(j) < k$,
        \begin{align*}
            v_t(j) &= \max_{i=1}^{N} v_{t-1}(i) \; a_{ij} \; b_j(o_t) \\
            bt_t(j) &= \argmax_{i=1}^{N} v_{t-1}(i) \; a_{ij} \; b_j(o_t) \\
            c_t(j) &=
            \begin{dcases}
                c_{t-1}(j) + 1 & {\rm if~} bt_t(j) = j \\
                0 & {\rm otherwise} \\
            \end{dcases}
        \end{align*}
        otherwise,
        \begin{align*}
            v_t(j) &= \max_{1 \le i \le N}^{i \ne j} v_{t-1}(i) \; a_{ij} \; b_j(o_t) \\
            bt_t(j) &= \argmax_{1 \le i \le N}^{i \ne j} v_{t-1}(i) \; a_{ij} \; b_j(o_t) \\
            c_t(j) &= 0
        \end{align*}
    \item \textbf{Termination:} (unchanged)
        \begin{align*}
            \textrm{The best score:}&~~ P* = \max_{i=1}^N v_T(i) \; a_{iF} \\
            \textrm{The start of backtrace:}&~~ q_T* = \argmax_{i=1}^N v_T(i) \; a_{iF}
        \end{align*}
\end{enumerate}

% ------------------------------------------------

\section*{Problem 4 Solution}

We want to find a $\mu$ that maximizes the likelihood function:
\[
    \mathcal{L}(\mu | \mathcal{X}) = \Pr(\mathcal{X} | \mu)
                                   = \prod_{x \in \mathcal{X}} \Pr(x; \mu)
                                   = \prod_{x \in \mathcal{X}} \sum_{y \in \rm\bf Y} \Pr(x, y; \mu)
\]
where $\mathcal{X}$ is the set of observations (first letter of grade for each
student), and $y \in \bf Y$ is an actual grade where $\mathbf{Y} = \{\rm AA, AB, BB, BC \}$.

We know the probability distribution of $y$ in terms of $\mu$:
\[
    y \sim P_\mu =
    \begin{dcases}
        0.25 & y = \rm AA \\
        \mu & y = \rm AB \\
        3\mu & y = \rm BB \\
        0.75 - 4\mu & y = \rm BC
    \end{dcases}
\]

We can maximize $\mathcal{L}(\mu | \mathcal{X})$ using EM algorithm, by
iteratively maximizing $Q(\mu, \mu^{(t-1)})$ which is the expectation of the
log-likelihood of ``complete data'' (observed + hidden) over the hidden
variable. That is, repeat (over $t$ with a randomly initialized $\mu^{(0)}$):
\[
    \mu^{(t)} = \argmax_{\mu} Q(\mu, \mu^{(t-1)})
\]
\begin{align*}
    Q(\mu, \mu^{(t-1)}) &= E_{\mathcal{Y}} \left[ \log \mathcal{L}(\mu | \mathcal{Z})
                               \middle| \mathcal{X}, \mu^{(t-1)}
                               \right] \tag{$\mathcal{Z} =$ student-wise zip$(\mathcal{X, Y})$}\\
                        &= E_{\mathcal{Y}} \left[ \log \Pr(\mathcal{Z} | \mu)
                               \middle| \mathcal{X}, \mu^{(t-1)} \right] \\
                        &= E_{\mathcal{Y}} \left[ \sum_{(x, y) \in \mathcal{Z}} \log \Pr(x, y| \mu)
                               \middle| \mathcal{X}, \mu^{(t-1)} \right] \\
                        &= \sum_{\mathcal{Y}} \Pr(\mathcal{Y} | \mathcal{X}, \mu^{(t-1)})
                                              \sum_{(x, y) \in \mathcal{Z}} \log \Pr(x, y| \mu) \\
                        &= \sum_{x \in \mathcal{X}} \sum_{y \in \bf Y} \Pr (y | x, \mu^{(t-1)})
                                                        \log \Pr(x, y| \mu) \tag{Margin too small!}
\end{align*}
Note that in the above expression, the only unknown variable is $\mu$.

\subsection*{Part (a)}

\[
    \Pr(x, y | \mu) =
    \begin{dcases}
        0.25 & x = A, y = AA \\
        \mu & x = A, y = AB \\
        3\mu & x = B, y = BB \\
        0.75 - 4\mu & x = B, y = BC \\
        0 & \rm otherwise
    \end{dcases}
\]

\[
    \Pr(y | x, \mu^{(t-1)}) = \frac{\Pr(x, y | \mu^{(t-1)})}{\Pr(x | \mu^{(t-1)})}
\]

\[
    \Pr(y | x, \mu^{(t-1)}) =
    \begin{dcases}
        \frac{0.25}{0.25 + \mu^{(t-1)}} & x = A, y = AA \\
        \frac{\mu^{(t-1)}}{0.25 + \mu^{(t-1)}} & x = A, y = AB \\
        \frac{3\mu^{(t-1)}}{0.75 - \mu^{(t-1)}} & x = B, y = BB \\
        \frac{0.75 - 4\mu^{(t-1)}}{0.75 - \mu^{(t-1)}} & x = B, y = BC \\
        0 & \rm otherwise
    \end{dcases}
\]

\subsection*{Part (b)}

Let $N$ be the total number of students, and $\alpha$ denote the number of
students that are assigned a grade starting with letter $A$. Thus $Q$ can be
rewriteen as:
\begin{align*}
    Q(\mu, \mu^{(t-1)}) =\; &\alpha\sum_{y \in \rm\bf Y} \Pr(y | x=A, \mu^{(t-1)}) \log \Pr(x=A, y | \mu) + \\
                          &(N-\alpha)\sum_{y \in \rm\bf Y} \Pr(y | x=B, \mu^{(t-1)}) \log \Pr(x=B, y | \mu)
\end{align*}
Ignoring negative infinity terms, we get:
\begin{align*}
    Q(\mu, \mu^{(t-1)}) =\; &\alpha\left( \frac{0.25}{0.25 + \mu^{(t-1)}} \log 0.25 + \frac{\mu^{(t-1)}}{0.25 + \mu^{(t-1)}} \log \mu \right) + \\
                          &(N-\alpha)\left( \frac{3\mu^{(t-1)}}{0.75 - \mu^{(t-1)}} \log 3\mu + \frac{0.75 - 4\mu^{(t-1)}}{0.75 - \mu^{(t-1)}} \log (0.75 - 4\mu) \right) \\
                        =\; &\alpha \frac{-0.5 \log 2 + \mu^{(t-1)} \log \mu}{0.25 + \mu^{(t-1)}} +
                          (N-\alpha) \frac{3\mu^{(t-1)} \log 3\mu + (0.75 - 4\mu^{(t-1)}) \log (0.75 - 4\mu)}{0.75 - \mu^{(t-1)}} \\
                        =\; &\frac{C + \alpha\mu^{(t-1)}\log\mu + (N-\alpha)3\mu^{(t-1)}\log\mu - 4\mu^{(t-1)}\log(0.75-4\mu)}{0.5^2 - (0.25 - \mu^{(t-1)})^2} \\
                        =\; &\frac{C + (3N-2\alpha)\mu^{(t-1)}\log\mu - 4\mu^{(t-1)}\log(0.75-4\mu)}{0.5^2 - (0.25 - \mu^{(t-1)})^2}
\end{align*}

To maximize $Q$, we differentiate with respect to $\mu$ and equate to zero:
\begin{align*}
    \frac{(3N-2\alpha)\mu^{(t-1)}/\mu - 4\mu^{(t-1)}(-4)/(0.75-4\mu)}{0.5^2 - (0.25 - \mu^{(t-1)})^2} &= 0 \\
    \frac{(3N-2\alpha)(0.75 - 4\mu) + 16\mu}{\mu(0.75 - 4\mu)} &= 0 \tag{$\mu^{(t-1)} > 0$} \\
    (3N-2\alpha)(0.75 - 4\mu) + 16\mu &= 0 \tag{$\mu \ne 0, 4\mu \ne 0.75$} \\
    \mu &= \frac{0.75(3N - 2\alpha)}{4(3N - 2\alpha - 4)}
\end{align*}

This implies that we can maximize $Q$ in a single iteration!?

\section*{Problem 5 Solution}

\subsection*{Part (a)}

We add another variable $c_t(j)$ which is the number of observations emitted by
state $j$ when emitting observation $o_t$ as the last among them.
\begin{enumerate}
    \item \textbf{Initialization:} For all $1 \le j \le N$,
        \begin{align*}
            v_1(j) &= a_{0j} \, L(j, 1) \, B(j, o_1) \\
            bt_1(j) &= 0 \\
            c_1(j) &= 1
        \end{align*}
    \item \textbf{Recursion:} For all $1 < t \le T$ and $1 \le j \le N$,
        \begin{align*}
            \ell_{t, \max} &= \min(t, \ell_{\max}) \\
            v_t(j) &= \max_{\substack{1 \le i \le N \\ 1 \le \ell \le \ell_{t, \max}}} v_{t-\ell}(i) \;
                          a_{ij} \, L(j, \ell) \, B(j, o_{t-\ell+1}, \cdots o_t) \\
            bt_t(j) &= \argmax_{\substack{i \\ 1 \le i \le N \\ 1 \le \ell \le \ell_{t, \max}}} (\bullet) \\
            c_t(j) &= \argmax_{\substack{\ell \\ 1 \le i \le N \\ 1 \le \ell \le \ell_{t, \max}}} (\bullet) \\
        \end{align*}
    \item \textbf{Termination:} (unchanged)
        \begin{align*}
            \textrm{The best score:}&~~ P* = \max_{i=1}^N v_T(i) \; a_{iF} \\
            \textrm{The start of backtrace:}&~~ q_T* = \argmax_{i=1}^N v_T(i) \; a_{iF}
        \end{align*}
\end{enumerate}

\subsection*{Part (b)}

We add another variable $c_t(j)$ which is the number of observations emitted by
state $j$ when emitting observation $o_t$ as the last among them.
\begin{enumerate}
    \item \textbf{Initialization:} For all $1 \le j \le N$,
        \begin{align*}
            \alpha_1(j) &= a_{0j} \, L(j, 1) \, B(j, o_1)
        \end{align*}
    \item \textbf{Recursion:} For all $1 < t \le T$ and $1 \le j \le N$,
        \begin{align*}
            \ell_{t, \max} &= \min(t, \ell_{\max}) \\
            \alpha_t(j) &= \sum_{i=1}^N \sum_{\ell=1}^{\ell_{t, \max}} \alpha_{t-\ell}(i) \;
                          a_{ij} \, L(j, \ell) \, B(j, o_{t-\ell+1}, \cdots o_t)
        \end{align*}
    \item \textbf{Termination:} (unchanged)
        \begin{align*}
            \Pr(O|\lambda) = \sum_{i=1}^N \alpha_T(i) \; a_{iF}
        \end{align*}
\end{enumerate}

\section*{Problem 6 Solution}

\subsection*{Part (a)}

\begin{enumerate}[(i)]
    \item If $w_1, w_2, \cdots w_n$ denote the words in an utterance $s$, and $w'_1,
        w'_2, \cdots w'_m$ denote the words in the prediction $s'$, then we can define
        the edit distance function, $\Psi$ as:
        \begin{equation*}
            \Psi(w_1, \cdots w_n; w'_1, \cdots w'_m) =
            \begin{dcases}
                \Psi(w_2, \cdots w_n; w'_2, \cdots w'_m)
                & w_1 = w'_1 \\
                \begin{split}
                    1 + \min(\Psi(w_2, \cdots w_n; w'_1, \cdots w'_m), \\
                         \Psi(w_1, \cdots w_n; w'_2, \cdots w'_m), \\
                         \Psi(w_2, \cdots w_n; w'_2, \cdots w'_m))
                \end{split}
                & \rm otherwise
            \end{dcases}
        \end{equation*}
        The three terms which are $\min$-ed indicate an addition, deletion,
        and substitution respectively on the predicted sequence.
    \item If $\operatorname{join}(w_1, w_2)$ indicate the joining of two words
        to form a compound word, then the new recurrence relation can be
        defined as:
        \begin{equation*}
            \Psi(w_1, \cdots w_n; w'_1, \cdots w'_m) =
            \begin{dcases}
                \Psi(w_2, \cdots w_n; w'_2, \cdots w'_m) & w_1 = w'_1 \\
                1 + \Psi(w_2, \cdots w_n; w'_3, \cdots w'_m)
                & w_1 = \operatorname{join}(w'_1, w'_2) \\
                1 + \Psi(w_3, \cdots w_n; w'_2, \cdots w'_m)
                & w'_1 = \operatorname{join}(w_1, w_2) \\
                \begin{split}
                    1 + \min(\Psi(w_2, \cdots w_n; w'_1, \cdots w'_m), \\
                         \Psi(w_1, \cdots w_n; w'_2, \cdots w'_m), \\
                         \Psi(w_2, \cdots w_n; w'_2, \cdots w'_m))
                \end{split}
                & \rm otherwise
            \end{dcases}
        \end{equation*}
    \item While checking for equality, it can also be checked for accoustic
        similarity and the associated cost can be discounted appropriately.
\end{enumerate}

\subsection*{Part (b)}

(Will update if time permits.)

\end{document}
